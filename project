from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
import os
import numpy as np
from imutils import paths
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing import image_dataset_from_directory
import matplotlib.pyplot as plt
%matplotlib inline 
import random

def func(x):
    ins1 = random.uniform(-200, 200)
    ins2 = random.uniform(-200, 200)
    ins3 = random.uniform(-200, 200)
    z = x.copy()
    for i in range(x.shape[0]):
      for j in range(x.shape[1]):
        for k in range(x.shape[2]):
          if k == 0:
            z[i, j, k] += ins1
          elif k == 1:
            z[i, j, k] += ins2
          elif k == 2:
            z[i, j, k] -= ins3
    return z
image_size = (341, 256)
batch_size = 64

train_dataset = image_dataset_from_directory('/content/drive/MyDrive/Colab Notebooks/aug/temp_train',
                                             subset = 'training',
                                             seed = 42,
                                             validation_split = 0.25,
                                             batch_size = batch_size,
                                             image_size = image_size)
validation_dataset = image_dataset_from_directory('/content/drive/MyDrive/Colab Notebooks/aug/temp_train',
                                             subset = 'validation',
                                             seed = 42,
                                             validation_split = 0.25,
                                             batch_size = batch_size,
                                             image_size = image_size)
test_dataset = image_dataset_from_directory('/content/drive/MyDrive/Colab Notebooks/aug/train',
                                             seed = 42,
                                             batch_size = batch_size,
                                             image_size = image_size)
                                             


from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(
    rescale=1./255,
                                   width_shift_range=0.25, 
                                   height_shift_range=0.25,
                                   brightness_range=[0.35,0.6],
                                   zoom_range=[0.7, 1.3],
                                   preprocessing_function = func,
                                   fill_mode = 'nearest'
)

valid_datagen = ImageDataGenerator(
    rescale=1./255,
                                   width_shift_range=0.25, 
                                   height_shift_range=0.25,
                                   brightness_range=[0.35,0.6],
                                   zoom_range=[0.7, 1.3],
                                   preprocessing_function = func,
                                   fill_mode = 'nearest'
)


from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
gen1_dir = '/content/drive/MyDrive/Colab Notebooks/aug/temp_train/cleaned'
gen2_dir = '/content/drive/MyDrive/Colab Notebooks/aug/temp_train/dirty'
train_C_dir = '/content/drive/MyDrive/Colab Notebooks/aug/train/cleaned'
train_D_dir = '/content/drive/MyDrive/Colab Notebooks/aug/train/dirty'
prefix1 = "cleaned."
prefix2 = "dirty."

def augmentor_func(gen_dir, content, prefix):
  for i in range(20):
    if i < 10:
      pic = load_img( content + '/000' + str(i) + '.jpg')
    else:
      pic = load_img( content + '/00' + str(i) + '.jpg')
    pic_array = img_to_array(pic)
    pic_array = pic_array.reshape((1,) + pic_array.shape)
    count = 0
    for batch in train_datagen.flow(pic_array, batch_size = 1, save_to_dir = gen_dir, save_prefix=prefix):
        count += 1
        if count == 100:
            break


augmentor_func(gen2_dir, train_D_dir, prefix2)
augmentor_func(gen1_dir, train_C_dir, prefix1)

from keras.models import load_model
from sklearn.datasets import load_files   
from keras.utils import np_utils
from keras.optimizers import Adam
from glob import glob
from keras import applications
from keras.preprocessing.image import ImageDataGenerator 
from keras import optimizers
from keras.models import Sequential,Model,load_model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D
from keras.callbacks import TensorBoard,ReduceLROnPlateau,ModelCheckpoint

base_model = applications.resnet50.ResNet50(weights='imagenet', include_top=False, input_shape= (341,256,3))



x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(10, activation='relu')(x)
x = Dropout(0.7)(x)
predictions = Dense(1, activation= 'sigmoid')(x)
model = Model(inputs = base_model.input, outputs = predictions)
opt = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)


print ("Model compiled")
model.summary()


callbacks = [tf.keras.callbacks.ModelCheckpoint()]
model.compile(loss='binary_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])
model.execute_eagerly = False


history = model.fit(train_dataset, 
                    validation_data=validation_dataset,
                    epochs=30,
                    verbose=2)
                    
plt.plot(history.history['accuracy'], label='Accuracy (training data)')
plt.plot(history.history['val_accuracy'], label='Accuracy (validation data)')
plt.ylabel('Accuracy value')
plt.xlabel('No. epoch')
plt.legend(loc="upper left")
plt.show()


import pickle
# model.save('/content/drive/MyDrive/Colab Notebooks/98.hdf5')
pickle.dump(history.history, open('/content/drive/MyDrive/Colab Notebooks/98.hist.pickle', 'wb'))

loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/98.hdf5')

scores = loaded_model.evaluate(test_dataset, verbose=1)
